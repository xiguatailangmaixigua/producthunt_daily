#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
import os
import logging
import re
from datetime import datetime
import pytz
from dotenv import load_dotenv
import urllib.parse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def clean_translation_content(content):
    """æ¸…ç†ç¿»è¯‘å†…å®¹ï¼Œå»æ‰"ç¬¬ä¸€æ¬¡ç›´è¯‘"å’Œ"ç¬¬äºŒæ¬¡æ„è¯‘"çš„æ ‡è®°ï¼Œåªä¿ç•™ç¬¬äºŒæ¬¡æ„è¯‘çš„å†…å®¹"""
    if not content:
        return content
    
    # æŸ¥æ‰¾ç¬¬äºŒæ¬¡æ„è¯‘çš„å†…å®¹
    second_translation_match = re.search(r'###\s*ç¬¬äºŒæ¬¡æ„è¯‘\s*([\s\S]+?)(?=###|$)', content)
    if second_translation_match:
        # è¿”å›ç¬¬äºŒæ¬¡æ„è¯‘çš„å†…å®¹ï¼Œå¹¶å»æ‰å¯èƒ½çš„å‰åç©ºç™½
        return second_translation_match.group(1).strip()
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¬¬äºŒæ¬¡æ„è¯‘ï¼Œåˆ™å»æ‰ç¬¬ä¸€æ¬¡ç›´è¯‘çš„æ ‡è®°
    content = re.sub(r'###\s*ç¬¬ä¸€æ¬¡ç›´è¯‘\s*', '', content)
    
    return content.strip()

def clean_url(url):
    """æ¸…ç†URLï¼Œå»æ‰æŸ¥è¯¢å‚æ•°"""
    if not url:
        return url
    parsed_url = urllib.parse.urlparse(url)
    clean_url = urllib.parse.urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, '', '', ''))
    return clean_url

def generate_chinese_markdown(products, date_str):
    """ç”Ÿæˆä¸­æ–‡ç‰ˆçš„Markdownæ–‡ä»¶"""
    logger.info("å¼€å§‹ç”Ÿæˆä¸­æ–‡Markdownæ–‡ä»¶...")
    
    markdown_content = f"# ProductHuntçƒ­é—¨åº”ç”¨ | {date_str}\n\n"
    
    for i, product in enumerate(products, 1):
        # è·å–äº§å“ä¿¡æ¯
        name = product.get('name', '')
        product_hunt_url = product.get('product_hunt_url', '')
        
        # æ¸…ç†URLï¼Œå»æ‰æŸ¥è¯¢å‚æ•°
        product_hunt_url = clean_url(product_hunt_url)
        visit_url = clean_url(product.get('visit_url', ''))
        
        # ä½¿ç”¨label_zhä½œä¸ºæ ‡è¯­
        label_zh = product.get('label_zh', product.get('label', ''))
        
        # ä½¿ç”¨content_zhä½œä¸ºä»‹ç»ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°å…¶ä»–å­—æ®µ
        if 'content_zh' in product:
            detailed_content = product.get('content_zh', '')
            # æ¸…ç†ç¿»è¯‘å†…å®¹
            detailed_content = clean_translation_content(detailed_content)
        else:
            # å‘åå…¼å®¹
            maker_introduction_zh = product.get('maker_introduction_zh', product.get('maker_introduction', ''))
            # é™åˆ¶maker_introductionçš„é•¿åº¦
            if maker_introduction_zh and len(maker_introduction_zh) > 500:
                maker_introduction_zh = maker_introduction_zh[:500] + "..."
            detailed_content = maker_introduction_zh
        
        topics_zh = product.get('topics_zh', product.get('topics', []))
        votes = product.get('votes', 0)
        is_featured = product.get('is_featured', False)
        created_at = product.get('created_at', '')
        icon = product.get('icon', '')
        image = product.get('image', '')
        
        # ç”ŸæˆMarkdownå†…å®¹
        markdown_content += f"## [{i}. {name}]({product_hunt_url})\n"
        markdown_content += f"**ç®€ä»‹**ï¼š{label_zh}\n"
        markdown_content += f"**åŠŸèƒ½**ï¼š{detailed_content}\n"
        markdown_content += f"**äº§å“ç½‘ç«™**: {visit_url}\n"
        markdown_content += f"**Product Hunt**: {product_hunt_url}\n\n"
        
        # æ·»åŠ å›¾ç‰‡
        markdown_content += f"![]({image})\n"
        
        # æ·»åŠ ä¸»é¢˜æ ‡ç­¾
        if topics_zh:
            if isinstance(topics_zh, list):
                markdown_content += f"**å…³é”®è¯**ï¼š{', '.join(topics_zh)}\n"
            else:
                markdown_content += f"**å…³é”®è¯**ï¼š{topics_zh}\n"
        
        # æ·»åŠ æŠ•ç¥¨æ•°
        markdown_content += f"**ç¥¨æ•°**: ğŸ”º{votes}\n"
        
        # æ·»åŠ æ˜¯å¦ç²¾é€‰
        markdown_content += f"**æ˜¯å¦ç²¾é€‰**ï¼š{'æ˜¯' if is_featured else 'å¦'}\n"
        
        # æ·»åŠ å‘å¸ƒæ—¶é—´
        markdown_content += f"**å‘å¸ƒæ—¶é—´**ï¼š{created_at}\n\n"
        
        # æ·»åŠ åˆ†éš”çº¿
        markdown_content += "---\n\n"
    
    # ç§»é™¤æœ€åä¸€ä¸ªåˆ†éš”çº¿
    if markdown_content.endswith("---\n\n"):
        markdown_content = markdown_content[:-5]
    
    # ä¿å­˜Markdownæ–‡ä»¶
    output_file_path = f"data/producthunt-daily-{date_str}.md"
    with open(output_file_path, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    logger.info(f"ä¸­æ–‡Markdownæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file_path}")
    
    return output_file_path

def main():
    try:
        logger.info("å¼€å§‹æ‰§è¡Œç”Ÿæˆä¸­æ–‡Markdownç¨‹åº...")
        
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        import argparse
        parser = argparse.ArgumentParser(description='ç”Ÿæˆä¸­æ–‡Markdownæ–‡ä»¶')
        parser.add_argument('--date', type=str, help='æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DD')
        args = parser.parse_args()
        
        if args.date:
            date_str = args.date
            logger.info(f"ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: {date_str}")
        else:
            # è·å–å½“å‰æ—¥æœŸ
            beijing_tz = pytz.timezone('Asia/Shanghai')
            current_time = datetime.now(beijing_tz)
            date_str = current_time.strftime('%Y-%m-%d')
            logger.info(f"ä½¿ç”¨å½“å‰æ—¥æœŸ: {date_str}")
        
        # è¯»å–ç¿»è¯‘åçš„JSONæ–‡ä»¶
        json_file_path = f"data/product_{date_str}_zh.json"
        if not os.path.exists(json_file_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
            return
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            products = json.load(f)
        
        logger.info(f"æˆåŠŸè¯»å– {len(products)} ä¸ªäº§å“ä¿¡æ¯")
        
        # ç”Ÿæˆä¸­æ–‡Markdownæ–‡ä»¶
        output_file_path = generate_chinese_markdown(products, date_str)
        
        logger.info(f"ä¸­æ–‡Markdownæ–‡ä»¶å·²ç”Ÿæˆ: {output_file_path}")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}", exc_info=True)
    finally:
        logger.info("ç”Ÿæˆä¸­æ–‡Markdownç¨‹åºæ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()
